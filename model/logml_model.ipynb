{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4afbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2  \n",
    "\n",
    "import dgl \n",
    "import torch \n",
    "import pandas as pd \n",
    "import pickle \n",
    "import pytorch_lightning as pl\n",
    "import random\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5824fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_pickle(f):\n",
    "    with open(f, 'rb') as fname:\n",
    "        node_dict = pickle.load(fname)\n",
    "    return node_dict\n",
    "\n",
    "def to_pickle(node_dict, f):\n",
    "    with open(f, 'wb') as fname:\n",
    "        pickle.dump(node_dict, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40e8e9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembling graph\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Set up model params\n",
    "#\n",
    "print(\"Assembling graph\")\n",
    "node_df = pd.read_csv(\"/n/holylfs06/LABS/mzitnik_lab/Lab/ruthjohnson/kg_paper_revision/connected_node_logml_df.csv\", sep='\\t')\n",
    "edge_df = pd.read_csv(\"/n/holylfs06/LABS/mzitnik_lab/Lab/ruthjohnson/kg_paper_revision/connected_edge_logml_df.csv\", sep='\\t')\n",
    "\n",
    "u = torch.tensor(edge_df['node_index_x'].tolist())\n",
    "v = torch.tensor(edge_df['node_index_y'].tolist())\n",
    "\n",
    "g = dgl.graph((u,v))\n",
    "graph_feature_df = pd.read_csv(\"/n/holylfs06/LABS/mzitnik_lab/Lab/ruthjohnson/kg_paper_revision/graph_feature_logml_df.csv\")\n",
    "g.ndata['feat'] = torch.tensor(graph_feature_df.values, dtype=torch.float32)\n",
    "\n",
    "ntype_list = node_df['ntype'].unique()\n",
    "ntype_dict = {}\n",
    "ntype_index_dict = {}\n",
    "i = 0\n",
    "for t in ntype_list:\n",
    "    ntype_dict[t] = i\n",
    "    ntype_index_dict[i] = t\n",
    "    i+=1 \n",
    "\n",
    "ntype_index_dict = {}\n",
    "ntype_index_dict['ATC'] = 0\n",
    "ntype_index_dict['ICD10CM'] = 2\n",
    "ntype_index_dict['LNC'] = 1\n",
    "ntype_index_dict['PHECODE'] = 2\n",
    "ntype_index_dict['RXNORM'] = 0\n",
    "ntype_index_dict['SNOMEDCT_US'] = 3\n",
    "ntype_index_dict['UMLS_CUI'] = 3\n",
    "\n",
    "etypes = edge_df['ntype_x'] + ':' + edge_df['ntype_y']\n",
    "etype_list = etypes.unique()\n",
    "etype_dict = {}\n",
    "i = 0 \n",
    "for t in etype_list:\n",
    "    if t.split(':')[1] in ['ATC', 'PHECODE', 'CPT']:\n",
    "        etype_dict[t] = 1\n",
    "    else:\n",
    "        etype_dict[t] = 0\n",
    "\n",
    "node_df['ntype_index'] = node_df['ntype'].map(ntype_index_dict)\n",
    "g.ndata['ntype'] = torch.tensor(node_df['ntype_index'].tolist(), dtype=torch.int32)\n",
    "g.edata['etype'] = torch.tensor((edge_df['ntype_x'] + ':' + edge_df['ntype_y']).map(etype_dict), dtype=torch.int32)\n",
    "\n",
    "#g = dgl.add_self_loop(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0e7f99e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a mapping for quick lookups\n",
    "source_target_pairs = edge_df[['node_index_x', 'node_index_y']].apply(tuple, axis=1)\n",
    "reverse_pairs = edge_df[['node_index_y', 'node_index_x']].apply(tuple, axis=1)\n",
    "\n",
    "# Create a dictionary for reverse edges\n",
    "reverse_map = dict(zip(source_target_pairs, edge_df['edge_index']))\n",
    "\n",
    "# Assign reverse edges using the reverse_pairs\n",
    "edge_df['reverse_edge_index'] = reverse_pairs.map(reverse_map)\n",
    "rev_id_dict = edge_df.set_index('edge_index')['reverse_edge_index'].to_dict()\n",
    "to_pickle(rev_id_dict, \"rev_edge_dict_logml.pkl\")\n",
    "\n",
    "rev_id_dict = open_pickle(\"/n/holylfs06/LABS/mzitnik_lab/Lab/ruthjohnson/kg_paper_revision/model/rev_edge_dict_logml.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e1ef6e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=10\n",
    "\n",
    "config = {\n",
    "    'num_layers': 2,\n",
    "    'n_neg': 1,\n",
    "    'batch_size': 500,\n",
    "    'sampler_n': 20, \n",
    "    'lr': 1e-4,\n",
    "    'in_feat': 128,\n",
    "    'out_feat': 128,\n",
    "    'head_size': 512,\n",
    "    'num_heads': 3, \n",
    "}\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6f808e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home01/ruthjohnson/.local/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /n/home01/ruthjohnson/venv_dgl/lib/python3.10/site-p ...\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "logger = CSVLogger(\"logs\", name=\"my_exp_name\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=n_epochs,\n",
    "                        log_every_n_steps=1,\n",
    "                        #precision=\"bf16-mixed\", #\"bf16-mixed\"\n",
    "                        accelerator=device,\n",
    "                        logger=logger\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a049a88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name        | Type                          | Params | Mode \n",
      "----------------------------------------------------------------------\n",
      "0 | gnn         | mini_hgt                      | 115 K  | train\n",
      "1 | accuracy    | BinaryAccuracy                | 0      | train\n",
      "2 | softmax     | Softmax                       | 0      | train\n",
      "3 | cos         | CosineSimilarity              | 0      | train\n",
      "4 | margin_loss | TripletMarginWithDistanceLoss | 0      | train\n",
      "----------------------------------------------------------------------\n",
      "115 K     Trainable params\n",
      "0         Non-trainable params\n",
      "115 K     Total params\n",
      "0.461     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6434213c868b42cb95845b0445b25ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1026\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:216\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:455\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:150\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:282\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    281\u001b[0m dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m batch, _, __ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py:134\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py:61\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py:78\u001b[0m, in \u001b[0;36m_MaxSizeCycle.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     out[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m/n/holylfs06/LABS/mzitnik_lab/Lab/ruthjohnson/kg_paper_revision/model/dataloader.py:113\u001b[0m, in \u001b[0;36mtyped_edge_dataloader.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample_eid \u001b[38;5;129;01min\u001b[39;00m eid_chunk_list:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# get list of neg samples (eid)\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     neg_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneg_sampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_eid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# id for src and dst node\u001b[39;00m\n",
      "File \u001b[0;32m/n/holylfs06/LABS/mzitnik_lab/Lab/ruthjohnson/kg_paper_revision/model/dataloader.py:91\u001b[0m, in \u001b[0;36mtyped_edge_dataloader.neg_sampler\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# sample from all possible dst nodes \u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m curr_dst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhomo_hg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_id\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# match negs by ntype\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dgl/heterograph.py:3507\u001b[0m, in \u001b[0;36mDGLGraph.out_edges\u001b[0;34m(self, u, form, etype)\u001b[0m\n\u001b[1;32m   3506\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DGLError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu contains invalid node IDs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 3507\u001b[0m src, dst, eid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_etype_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m form \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dgl/heterograph_index.py:661\u001b[0m, in \u001b[0;36mHeteroGraphIndex.out_edges\u001b[0;34m(self, etype, v)\u001b[0m\n\u001b[1;32m    660\u001b[0m edge_array \u001b[38;5;241m=\u001b[39m _CAPI_DGLHeteroOutEdges_2(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mint\u001b[39m(etype), F\u001b[38;5;241m.\u001b[39mto_dgl_nd(v))\n\u001b[0;32m--> 661\u001b[0m src \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mfrom_dgl_nd(\u001b[43medge_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    662\u001b[0m dst \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mfrom_dgl_nd(edge_array(\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m EdgePredModel(g, config)\n\u001b[1;32m      4\u001b[0m data_module \u001b[38;5;241m=\u001b[39m edge_pred_dataloader(homo_hg\u001b[38;5;241m=\u001b[39mg, homo_hg_dict\u001b[38;5;241m=\u001b[39mconfig, rev_edge_dict\u001b[38;5;241m=\u001b[39mrev_id_dict)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "from dataloader import edge_pred_dataloader\n",
    "\n",
    "model = EdgePredModel(g, config)\n",
    "data_module = edge_pred_dataloader(homo_hg=g, homo_hg_dict=config, rev_edge_dict=rev_id_dict)\n",
    "\n",
    "trainer.fit(model=model, datamodule=data_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "877600cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.gnn.state_dict(), \"logml_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fb96c963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load('logml_model.pt')\n",
    "model.gnn.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2bad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_input_nodes, edge_graph, edge_blocks, edge_batch_samples = next(iter(data_module.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "72480c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = []\n",
    "neg_list = []\n",
    "anch_list = []\n",
    "\n",
    "for s in edge_batch_samples:\n",
    "    d_anch = s[0]\n",
    "    d_pos = s[1]\n",
    "    d_neg = s[2]\n",
    "    anch_embed = model.h_dict[d_anch[0].item()]\n",
    "    pos_embed = model.h_dict[d_pos[0].item()]\n",
    "    neg_embed = model.h_dict[d_neg[0].item()] \n",
    "    \n",
    "    anch_list.append(anch_embed)\n",
    "    pos_list.append(pos_embed)\n",
    "    neg_list.append(neg_embed)       \n",
    "\n",
    "anch_vec = torch.stack(anch_list)\n",
    "pos_vec = torch.stack(pos_list)\n",
    "neg_vec = torch.stack(neg_list)\n",
    "\n",
    "loss = model.margin_loss(anch_vec, pos_vec, neg_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0229ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_pos = model.cos(pos_vec, anch_vec)\n",
    "d_neg = model.cos(neg_vec, anch_vec)\n",
    "correct = (d_pos < d_neg).sum().item()\n",
    "acc = correct/len(anch_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "09587d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1239,  0.1855, -0.0995,  ..., -0.1021,  0.0706, -0.0233],\n",
       "        [ 0.1179,  0.1807, -0.0942,  ..., -0.1019,  0.0662, -0.0186],\n",
       "        [ 0.1240,  0.1861, -0.0882,  ..., -0.1032,  0.0585, -0.0069],\n",
       "        ...,\n",
       "        [ 0.1287,  0.1866, -0.0981,  ..., -0.1051,  0.0712, -0.0090],\n",
       "        [ 0.1129,  0.1801, -0.1057,  ..., -0.0861,  0.0664, -0.0050],\n",
       "        [ 0.1210,  0.1889, -0.0937,  ..., -0.1011,  0.0632, -0.0190]],\n",
       "       grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anch_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b35f5c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "3825",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m edge_input_nodes, edge_graph, edge_blocks, edge_batch_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(data_module\u001b[38;5;241m.\u001b[39mtrain_dataloader()))\n\u001b[0;32m----> 2\u001b[0m edge_loss_score \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_batch_samples\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[72], line 53\u001b[0m, in \u001b[0;36mEdgePredModel.edge_loss\u001b[0;34m(self, batch_samples)\u001b[0m\n\u001b[1;32m     51\u001b[0m d_pos \u001b[38;5;241m=\u001b[39m s[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     52\u001b[0m d_neg \u001b[38;5;241m=\u001b[39m s[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m---> 53\u001b[0m anch_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43md_anch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     54\u001b[0m pos_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh_dict[d_pos[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()]\n\u001b[1;32m     55\u001b[0m neg_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh_dict[d_neg[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()] \n",
      "\u001b[0;31mKeyError\u001b[0m: 3825"
     ]
    }
   ],
   "source": [
    "#edge_input_nodes, edge_graph, edge_blocks, edge_batch_samples = next(iter(data_module.train_dataloader()))\n",
    "edge_loss_score = model.edge_loss(edge_batch_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c7590801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   22,    48,    59,  ..., 26207, 44632,  6581])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_input_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "74b175aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.h_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "59ca7108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl \n",
    "import torch \n",
    "import pytorch_lightning as pl\n",
    "import gc\n",
    "import torchmetrics\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# set seeds\n",
    "torch.manual_seed(0)\n",
    "import random\n",
    "random.seed(0)\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "\"\"\"\n",
    "    config_dict: in_feat, out_feat, head_size_1, head_size_2, num_heads_1, num_heads_2, dropout\n",
    "\"\"\"\n",
    "class mini_hgt(pl.LightningModule):\n",
    "    def __init__(self, config_dict):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = config_dict['num_layers']\n",
    "        in_feat = config_dict['in_feat']\n",
    "        #num_heads = config_dict['num_heads']\n",
    "        num_heads = 1\n",
    "        out_feat = config_dict['out_feat']\n",
    "                            \n",
    "        self.convs = torch.nn.ModuleList()\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            in_dim = in_feat if i == 0 else (in_feat * num_heads)\n",
    "            #conv = dgl.nn.pytorch.conv.GATConv(in_dim, out_feat, num_heads)\n",
    "            conv = dgl.nn.pytorch.conv.SAGEConv(in_dim, out_feat, \"pool\")\n",
    "            self.convs.append(conv)\n",
    "        self.linear = torch.nn.Linear(in_feat * num_heads, out_feat)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        for i in range(self.num_layers):\n",
    "            b = blocks[i]\n",
    "            x = self.convs[i](b, x)\n",
    "\n",
    "            # if not last layer\n",
    "            if i < self.num_layers - 1:\n",
    "                x = self.relu(x)\n",
    "\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c6006908",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgePredModel(pl.LightningModule):\n",
    "    def __init__(self, homo_hg, hgt_config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = hgt_config['lr']\n",
    "        self.homo_hg = homo_hg\n",
    "        self.gnn = mini_hgt(hgt_config)\n",
    "\n",
    "        self.accuracy = torchmetrics.classification.Accuracy(task='binary')\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        self.margin_loss = nn.TripletMarginWithDistanceLoss(distance_function=self.cos, margin=0.2)\n",
    "\n",
    "        self.h_dict = {}\n",
    "                 \n",
    "    def forward(self, input_nodes, blocks):\n",
    "\n",
    "        x_graph = blocks[0].srcdata['feat']\n",
    "\n",
    "        # returns dictionary of embeddings\n",
    "        h = self.gnn(blocks, x_graph)\n",
    "\n",
    "        for ind, i in zip(input_nodes.tolist(), range(0, h.shape[0])):\n",
    "            self.h_dict[ind] = h[i]\n",
    "            \n",
    "                    \n",
    "    def training_step(self, train_batch, _):\n",
    "        \n",
    "        edge_input_nodes, edge_graph, edge_blocks, edge_batch_samples = train_batch\n",
    "        \n",
    "        # edges\n",
    "        self.forward(edge_input_nodes, edge_blocks)\n",
    "        edge_loss_score, acc = self.edge_loss(edge_batch_samples)\n",
    "\n",
    "        self.log('train_loss_edge', edge_loss_score, prog_bar=True, batch_size=len(edge_batch_samples))\n",
    "        self.log('train_acc_edge', acc, prog_bar=True, batch_size=len(edge_batch_samples)) \n",
    "        \n",
    "        # reset embeddings dict\n",
    "        self.h_dict = {}\n",
    "            \n",
    "        return edge_loss_score\n",
    "        \n",
    "    def edge_loss(self, batch_samples):\n",
    "        pos_list = []\n",
    "        neg_list = []\n",
    "        anch_list = []\n",
    "\n",
    "        for s in batch_samples:\n",
    "            d_anch = s[0]\n",
    "            d_pos = s[1]\n",
    "            d_neg = s[2]\n",
    "            anch_embed = self.h_dict[d_anch[0].item()]\n",
    "            pos_embed = self.h_dict[d_pos[0].item()]\n",
    "            neg_embed = self.h_dict[d_neg[0].item()] \n",
    "            \n",
    "            anch_list.append(anch_embed)\n",
    "            pos_list.append(pos_embed)\n",
    "            neg_list.append(neg_embed)       \n",
    "\n",
    "        anch_vec = torch.stack(anch_list)\n",
    "        pos_vec = torch.stack(pos_list)\n",
    "        neg_vec = torch.stack(neg_list)\n",
    "\n",
    "        loss = self.margin_loss(anch_vec, pos_vec, neg_vec)\n",
    "\n",
    "        d_pos = model.cos(pos_vec, anch_vec)\n",
    "        d_neg = model.cos(neg_vec, anch_vec)\n",
    "        correct = (d_pos < d_neg).sum().item()\n",
    "        acc = correct/len(anch_vec)\n",
    "\n",
    "        return loss, acc\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        param_list = [{'params': self.gnn.parameters()}]\n",
    "        optimizer = torch.optim.Adam(param_list, lr=self.lr)\n",
    "        #lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5, verbose=True)\n",
    "        return [optimizer] #, [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4942d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_dgl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
